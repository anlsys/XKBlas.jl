@article{montoison-orban-2023,
  author  = {Montoison, Alexis and Orban, Dominique},
  title   = {{Krylov.jl: A Julia basket of hand-picked Krylov methods}},
  journal = {Journal of Open Source Software},
  volume  = {8},
  number  = {89},
  pages   = {5187},
  year    = {2023},
  doi     = {10.21105/joss.05187}
}

@misc{blas-spec,
	author       = "{Basic Linear Algebra Subprograms Technical (BLAST) Forum}",
	title        = "Basic Linear Algebra Subprograms (BLAS) Technical Forum Standard",
	howpublished = "\url{https://netlib.org/blas/blast-forum/blas-report.pdf}",
	month        = "August",
	year         = "2001",
	note         = "Accessed: Nov. 04, 2024"
}

@misc{cublas,
	author = "{NVIDIA® CUDA™}",
	title = {CUDA CUBLAS Library - Version 1.0},
	howpublished = "\url{https://developer.download.nvidia.com/compute/cuda/1.0/CUBLAS_Library_1.0.pdf}",
	year		= "2007",
	month		= "June",
	note        = "Accessed: Nov. 05, 2024",
}
@misc{rocblas,
  author       = {AMD ROCm Team},
  title        = {rocBLAS: AMD's BLAS Library for the ROCm Platform},
  year         = {2025},
  url          = {https://rocm.docs.amd.com/projects/rocBLAS/en/latest/},
  note         = {Accessed: 2025-09-29}
}

@misc{onemkl,
  author       = {Intel Corporation},
  title        = {Intel® oneAPI Math Kernel Library (oneMKL)},
  year         = {2025},
  url          = {https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html},
  note         = {Accessed: 2025-09-29}
}

@InProceedings{mumps,
	author="Amestoy, Patrick R. and others",
	title="{MUMPS:} A General Purpose Distributed Memory Sparse Solver",
	booktitle="Applied Parallel Computing. New Paradigms for HPC in Industry and Academia",
	year="2001",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="121--130",
	isbn="978-3-540-70734-9"
}

@article{aurora-gemm,
  title={Millions of Matrix-Multiplications: GEMM Variations on Aurora},
  author={Colleen Bertoni and Thomas Applencourt and Longfei Gao and Ti Leggett},
  journal={2025 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  year={2025},
  pages={850-856},
  url={https://api.semanticscholar.org/CorpusID:280652330}
}

@inproceedings{pytorch-compile,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
title = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640366},
doi = {10.1145/3620665.3640366},
abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {929–947},
numpages = {19},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@misc{anthropic-claude-sonnet-45,
  author       = {Anthropic},
  title        = {Claude Sonnet 4.5},
  year         = {2025},
  url          = {https://www.anthropic.com},
  note         = {Large language model. Accessed January 30, 2026}
}


@article{hpcg,
  title={HPCG benchmark: a new metric for ranking high performance computing systems},
  author={Dongarra, Jack and Heroux, Michael A and Luszczek, Piotr}
}

@article{ssmc,
author = {Davis, Timothy A. and Hu, Yifan},
title = {The university of Florida sparse matrix collection},
year = {2011},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/2049662.2049663},
doi = {10.1145/2049662.2049663},
abstract = {We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing and managing the Collection, from MATLAB™, Mathematica™, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task.},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {1},
numpages = {25},
keywords = {Graph drawing, multilevel algorithms, performance evaluation, sparse matrices}
}

@misc{h100,
  title        = {{NVIDIA H100 GPU Datasheet}},
  author       = {{NVIDIA Corporation}},
  howpublished = {Online},
  year         = {2025},
  note         = {Accessed: 2026-01-12},
  url          = {https://resources.nvidia.com/en-us-gpu-resources/h100-datasheet-24306},
  organization = {NVIDIA Corporation}
}


@inproceedings{kaapi,
author = {Gautier, Thierry and Besseron, Xavier and Pigeon, Laurent},
year = {2007},
month = {07},
pages = {15-23},
title = {KAAPI: A thread scheduling runtime system for data flow computations on cluster of multi-processors},
journal = {PASCO'07: Proceedings of the 2007 International Workshop on Parallel Symbolic Computation},
doi = {10.1145/1278177.1278182}
}

@phdthesis{pereira23,
  TITLE = {{Efficient Use of Task-based Parallelism in HPC Parallel Applications}},
  AUTHOR = {Pereira, Romain},
  URL = {https://theses.hal.science/tel-04466797},
  NUMBER = {2023ENSL0097},
  SCHOOL = {{Ecole normale sup{\'e}rieure de lyon - ENS LYON}},
  YEAR = {2023},
  MONTH = Nov,
  KEYWORDS = {MPI ; OpenMP ; Task ; Programming Model ; High Performance Computing ; MPI ; OpenMP ; T{\^a}che ; Mod{\`e}le de Programmation ; Calcul Haute Performance},
  TYPE = {Theses},
  PDF = {https://theses.hal.science/tel-04466797v1/file/PEREIRA_Romain_2023ENSL0097_These.pdf},
  HAL_ID = {tel-04466797},
  HAL_VERSION = {v1},
}

@misc{krasowska2025reducing,
  author       = {Krasowska, David},
  title        = {Reducing Complexity in Scalable Numerical Computing with cuNumeric.jl},
  howpublished = {Poster presented at the 2025 Department of Energy Computational Science Graduate Fellowship (CSGF) Program Review},
  year         = {2025},
  month        = {Apr},
  note         = {Northwestern University, Evanston, IL},
  url          = {https://krasow.dev/assets/documents/posters/2025_csgf_krasoska.pdf}
}

@inproceedings{legion,
 title={Legion: programming distributed heterogeneous architectures with logical regions},
 author={Michael Bauer},
 year={2014},
 url={https://api.semanticscholar.org/CorpusID:11740942}
}

@article{AlMouhamed2023SpMV,
  author  = {Mayez Al-Mouhamed and Lutfi Firdaus and Ayaz H. Khan and Nazeeruddin Mohammad},
  title   = {SpMV and BiCG-Stab sparse solver on Multi-GPUs for reservoir simulation},
  journal = {Multimedia Tools and Applications},
  year    = {2023},
  volume  = {83},
  number  = {8},
  pages   = {23563--23597},
  doi     = {10.1007/s11042-023-16185-0},
}

@INPROCEEDINGS{XKBlas,
	author={Gautier, Thierry and Lima, João V. F.},
	booktitle={2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)}, 
	title={XKBlas: a High Performance Implementation of BLAS-3 Kernels on Multi-GPU Server}, 
	year={2020},
	volume={},
	number={},
	pages={1-8},
	keywords={Task analysis;Graphics processing units;Libraries;Kernel;Layout;Runtime;Matrix decomposition;Multi-GPU;BLAS;Task Parallelism},
	doi={10.1109/PDP50117.2020.00008}}

@misc{cublasXT,
	title = {{cuBLAS -- Using the cuBLASXt API}},
	author       = "{NVIDIA® CUDA™}",
	howpublished = "\url{https://docs.nvidia.com/cuda/cublas/}",
	year         = 2024,
	note         = "Accessed: Oct. 21, 2024, Updated: Sep 30, 2024."
}

@article{parsec24,
	author = {Aurelien Bouteiller and Thomas Herault and Qinglei Cao and Joseph Schuchart and George Bosilca}, 
	title ={{PaRSEC:} Scalability, flexibility, and hybrid architecture support for task-based applications in {ECP}},
	journal = {The International Journal of High Performance Computing Applications},
	volume = {39}, 
	number = {1}, 
	pages = {10943420241290520},
	year = {2024}, 
	doi = {10.1177/10943420241290520},
	URL = { https://doi.org/10.1177/10943420241290520 },
	eprint = { https://doi.org/10.1177/10943420241290520 },
	abstract = { This paper highlights the most significant enhancements made to PaRSEC, a scalable task-based runtime system designed for hybrid machines, during the Exascale Computing Project (ECP). The enhancements focus on expanding the capabilities of PaRSEC to address the evolving landscape of parallel computing. Notable achievements include the integration of support for three major types of accelerators (NVIDIA, AMD, and Intel GPUs), the refinement and increased flexibility of the communication subsystem, and the introduction of new programming interfaces tailored for irregular applications. Additionally, the project resulted in the development of powerful debugging and performance analysis tools aimed at assisting users in understanding and optimizing their applications. We present a comprehensive demonstration of these advancements through a series of benchmarks and applications within ECP and beyond, thereby showcasing the enhanced capabilities of PaRSEC across the diverse architectures within the ECP, providing valuable insights into the runtime system’s adaptability and performance across varied computing environments. }
}


@INPROCEEDINGS{Chameleon,
	author={Beaumont, Olivier and Duchon, Philippe and Eyraud-Dubois, Lionel and Langou, Julien and Vérité, Mathieu},
	booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
	title={Symmetric Block-Cyclic Distribution: Fewer Communications Leads to Faster Dense Cholesky Factorization}, 
	year={2022},
	volume={},
	number={},
	pages={1-15},
	keywords={Runtime;Upper bound;Symmetric matrices;Scheduling algorithms;Scalability;Libraries;Resource management;Algorithms for numerical methods and algebraic systems;Load balancing and scheduling algorithms},
	doi={10.1109/SC41404.2022.00034}}

@article{MAGMA24,
	author = {Ahmad Abdelfattah and Natalie Beams and Robert Carson and Pieter Ghysels and Tzanio Kolev and Thomas Stitt and Arturo Vargas and Stanimire Tomov and Jack Dongarra},
	title ={MAGMA: Enabling exascale performance with accelerated BLAS and LAPACK for diverse GPU architectures},	
	journal = {The International Journal of High Performance Computing Applications},
	volume = {38},
	number = {5},
	pages = {468-490},
	year = {2024},
	doi = {10.1177/10943420241261960},	
	URL = {https://doi.org/10.1177/10943420241261960},
	eprint = {https://doi.org/10.1177/10943420241261960},
	abstract = { MAGMA (Matrix Algebra for GPU and Multicore Architectures) is a pivotal open-source library in the landscape of GPU-enabled dense and sparse linear algebra computations. With a repertoire of approximately 750 numerical routines across four precisions, MAGMA is deeply ingrained in the DOE software stack, playing a crucial role in high-performance computing. Notable projects such as ExaConstit, HiOP, MARBL, and STRUMPACK, among others, directly harness the capabilities of MAGMA. In addition, the MAGMA development team has been acknowledged multiple times for contributing to the vendors’ numerical software stacks. Looking back over the time of the Exascale Computing Project (ECP), we highlight how MAGMA has adapted to recent changes in modern HPC systems, especially the growing gap between CPU and GPU compute capabilities, as well as the introduction of low precision arithmetic in modern GPUs. We also describe MAGMA’s direct impact on several ECP projects. Maintaining portable performance across NVIDIA and AMD GPUs, and with current efforts toward supporting Intel GPUs, MAGMA ensures its adaptability and relevance in the ever-evolving landscape of GPU architectures. }
}

@inproceedings{BLASX,
	author = {Wang, Linnan and Wu, Wei and Xu, Zenglin and Xiao, Jianxiong and Yang, Yi},
	title = {BLASX: A High Performance Level-3 BLAS Library for Heterogeneous Multi-GPU Computing},
	year = {2016},
	isbn = {9781450343619},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2925426.2926256},
	doi = {10.1145/2925426.2926256},
	abstract = {Basic Linear Algebra Subprograms (BLAS) are a set of low level linear algebra kernels widely adopted by applications involved with the deep learning and scientific computing. The massive and economic computing power brought forth by the emerging GPU architectures drives interest in implementation of compute-intensive level 3 BLAS on multi-GPU systems. In this paper, we investigate existing multi-GPU level 3 BLAS and present that 1) issues, such as the improper load balancing, inefficient communication, insufficient GPU stream level concurrency and data caching, impede current implementations from fully harnessing heterogeneous computing resources; 2) and the inter-GPU Peer-to-Peer(P2P) communication remains unexplored. We then present BLASX: a highly optimized multi-GPU level-3 BLAS. We adopt the concepts of algorithms-by-tiles treating a matrix tile as the basic data unit and operations on tiles as the basic task. Tasks are guided with a dynamic asynchronous runtime, which is cache and locality aware. The communication cost under BLASX becomes trivial as it perfectly overlaps communication and computation across multiple streams during asynchronous task progression. It also takes the current tile cache scheme one step further by proposing an innovative 2-level hierarchical tile cache, taking advantage of inter-GPU P2P communication. As a result, linear speedup is observable with BLASX under multi-GPU configurations; and the extensive benchmarks demonstrate that BLASX consistently outperforms the related leading industrial and academic implementations such as cuBLAS-XT, SuperMatrix, MAGMA.},
	booktitle = {Proceedings of the 2016 International Conference on Supercomputing},
	articleno = {20},
	numpages = {11},
	keywords = {BLAS, Cache Hierarchy, MultiGPU, Runtime Scheduler, Tile Algorithms},
	location = {Istanbul, Turkey},
	series = {ICS '16}
}

@article{SLATE,
	author = {Mark Gates and Ahmad Abdelfattah and Kadir Akbudak and Mohammed Al Farhan and Rabab Alomairy and Daniel Bielich and Treece Burgess and Sébastien Cayrols and Neil Lindquist and Dalal Sukkari and Asim YarKhan},
	title ={Evolution of the {SLATE} linear algebra library},
	journal = {The International Journal of High Performance Computing Applications},
	volume = {39},
	number = {1},
	pages = {3--17},
	year = {2025},
	doi = {10.1177/10943420241286531},
	URL = { 
	https://doi.org/10.1177/10943420241286531
	},
	eprint = { 
	https://doi.org/10.1177/10943420241286531
	}
	,
	abstract = { SLATE (Software for Linear Algebra Targeting Exascale) is a distributed, dense linear algebra library targeting both CPU-only and GPU-accelerated systems, developed over the course of the Exascale Computing Project (ECP). While it began with several documents setting out its initial design, significant design changes occurred throughout its development. In some cases, these were anticipated: an early version used a simple consistency flag that was later replaced with a full-featured consistency protocol. In other cases, performance limitations and software and hardware changes prompted a redesign. Sequential communication tasks were parallelized; host-to-host MPI calls were replaced with GPU device-to-device MPI calls; more advanced algorithms such as Communication Avoiding LU and the Random Butterfly Transform (RBT) were introduced. Early choices that turned out to be cumbersome, error prone, or inflexible have been replaced with simpler, more intuitive, or more flexible designs. Applications have been a driving force, prompting a lighter weight queue class, nonuniform tile sizes, and more flexible MPI process grids. Of paramount importance has been building a portable library that works across several different GPU architectures – AMD, Intel, and NVIDIA – while keeping a clean and maintainable codebase. Here we explore the evolving design choices and their effects, both in terms of performance and software sustainability. }
}


@INPROCEEDINGS {PARALIA,
	author = { Anastasiadis, Petros and Papadopoulou, Nikela and Koziris, Nectarios and Goumas, Georgios },
	booktitle = { 2024 IEEE International Conference on Cluster Computing {(CLUSTER)} },
	title = {{ Uncut-GEMMs: Communication-Aware Matrix Multiplication on Multi-GPU Nodes }},
	year = {2024},
	volume = {},
	ISSN = {},
	pages = {143--154},
	abstract = { General Matrix Multiplication (GEMM) is one of the most common kernels in high-performance computing (HPC) and machine-learning (ML) applications, frequently dominating their execution time, rendering its performance vital. As multi-GPU nodes have become common in modern HPC systems, GEMM is usually offloaded on GPUs as its compute-intensive nature is a good match for their architecture. On the other hand, despite the GEMM kernel itself being usually compute-bound, execution on multi-GPU systems also requires fine-grained communication and task scheduling to achieve optimal performance. While numerous multi-GPU level-3 BLAS libraries have faced these issues in the past, they are bound by older design concepts that are not necessarily applicable to modern multi-GPU clusters, resulting in considerable deviation from peak performance. In this work, we thoroughly analyze the current challenges regarding data movement, caching, and overlap of multi-GPU GEMM, and the shortcomings of previous solutions, and provide a fresh approach to multi-GPU GEMM optimization. We devise a static scheduler for GEMM, enabling a variety of algorithmic, communication, and auto-tuning optimizations, and integrate those in an end-to-end open-source multi-GPU GEMM library. Our library is evaluated on a multi-GPU NVIDIA HGX system with 8 NVIDIA A100 GPUs, achieving on average a 1.37x and 1.29x performance improvement over the state-of-the-art multi-GPU GEMM libraries, for double and single precision, respectively. },
	keywords = {Schedules;Processor scheduling;Shape;Scalability;Throughput;Routing;Libraries;Kernel;Optimization;Streams},
	doi = {10.1109/CLUSTER59578.2024.00020},
	url = {https://doi.ieeecomputersociety.org/10.1109/CLUSTER59578.2024.00020},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month =sep}

@article{tile-blas,
	title = {A class of parallel tiled linear algebra algorithms for multicore architectures},
	journal = {Parallel Computing},
	volume = {35},
	number = {1},
	pages = {38--53},
	year = {2009},
	issn = {0167-8191},
	doi = {https://doi.org/10.1016/j.parco.2008.10.002},
	url = {https://www.sciencedirect.com/science/article/pii/S0167819108001117},
	author = {Alfredo Buttari and Julien Langou and Jakub Kurzak and Jack Dongarra},
	keywords = {Linear algebra, Mathematical software, High performance computing, Multicore},
	abstract = {As multicore systems continue to gain ground in the high performance computing world, linear algebra algorithms have to be reformulated or new algorithms have to be developed in order to take advantage of the architectural features on these new processors. Fine grain parallelism becomes a major requirement and introduces the necessity of loose synchronization in the parallel execution of an operation. This paper presents algorithms for the Cholesky, LU and QR factorization where the operations can be represented as a sequence of small tasks that operate on square blocks of data. These tasks can be dynamically scheduled for execution based on the dependencies among them and on the availability of computational resources. This may result in out of order execution of tasks which will completely hide the presence of intrinsically sequential tasks in the factorization. Performance comparisons are presented with LAPACK algorithms where parallelism can only be exploited at the level of the BLAS operations and vendor implementations.}
}
